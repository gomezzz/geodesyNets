{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Notebook\n",
    "\n",
    "First the imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josch\\miniconda3\\envs\\geodesynet\\lib\\site-packages\\pyvista\\themes.py:131: PyVistaDeprecationWarning: use \"dark\" instead of \"night\" theme\n",
      "  warnings.warn('use \"dark\" instead of \"night\" theme', PyVistaDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices  1\n",
      "__pyTorch VERSION: 1.13.1\n",
      "__CUDNN VERSION: 8500\n",
      "__Number CUDA Devices: 1\n",
      "Active CUDA Device: GPU 0\n",
      "Setting default tensor type to Float32\n",
      "Will use device  cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Import our module containing helper functions\n",
    "import gravann\n",
    "import gravann.polyhedral\n",
    "\n",
    "# Core imports\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import os\n",
    "from collections import deque\n",
    "\n",
    "# pytorch\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "# plotting stuff\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "%matplotlib notebook\n",
    "\n",
    "# Ensure that changes in imported module (gravann most importantly) are autoreloaded\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# If possible enable CUDA\n",
    "gravann.enableCUDA()\n",
    "gravann.fixRandomSeeds()\n",
    "device = os.environ[\"TORCH_DEVICE\"]\n",
    "print(\"Will use device \",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Loading the ground truth (both mascon and polyhedral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen body: eros\n",
      "Loaded Polyhedral Mesh Data:\n",
      "Loaded the Mesh of eros\n",
      "Number of vertices: 7374\n",
      "Number of faces (triangles): 14744\n",
      "Calculated Density: 3.394506131751055\n"
     ]
    }
   ],
   "source": [
    "##################### PLEASE SPECIFY THIS ####################\n",
    "# one of \"eros\", \"bennu\", \"itokawa\", \"churyumov-gerasimenko\", \"planetesimal\", \"torus\", \"bennu_nu\", \"itokawa_nu\", \"planetesimal_nu\"\n",
    "SAMPLE_NAME = \"eros\"\n",
    "# can be freely chosen\n",
    "MODEL_NAME = \"first\"\n",
    "# either 'mascon' or 'polyhedral'\n",
    "MODEL_METHOD = 'mascon'\n",
    "\n",
    "# Load an already trained and saved model (e.g. for re-training)?\n",
    "LOAD_TRAINED = False\n",
    "##############################################################\n",
    "\n",
    "MODEL_FILENAME = f\"models_polyhedral/{SAMPLE_NAME}_{MODEL_METHOD}_{MODEL_NAME}.mdl\"\n",
    "\n",
    "mesh_vertices, mesh_faces = gravann.load_polyhedral_mesh(SAMPLE_NAME)\n",
    "density = gravann.polyhedral.calculate_density(mesh_vertices, mesh_faces)\n",
    "\n",
    "# Only for the legacy plots required\n",
    "mascon_points, mascon_masses = gravann.load_mascon_data(SAMPLE_NAME)\n",
    "\n",
    "print(f\"Chosen body: {SAMPLE_NAME}\")\n",
    "print(f\"Loaded Polyhedral Mesh Data:\")\n",
    "print(f\"Loaded the Mesh of {SAMPLE_NAME}\")\n",
    "print(f\"Number of vertices: {len(mesh_vertices)}\")\n",
    "print(f\"Number of faces (triangles): {len(mesh_faces)}\")\n",
    "print(f\"Calculated Density: {density}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Defining the network architecture\n",
    "We here use functions from our module as to not clutter the notebook, but the code is rather straight forward: a FFNN with some options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 23\u001B[0m\n\u001B[0;32m     19\u001B[0m     model\u001B[38;5;241m.\u001B[39mload_state_dict(torch\u001B[38;5;241m.\u001B[39mload(MODEL_FILENAME))\n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m# Once a model is loaded the learned constant c (named kappa in the paper) is unknown\u001B[39;00m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# and must be relearned (ideally it should also be saved at the end of the training as it is a learned parameter)\u001B[39;00m\n\u001B[1;32m---> 23\u001B[0m c \u001B[38;5;241m=\u001B[39m \u001B[43mgravann\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_c_for_model_v2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mMODEL_METHOD\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmesh_vertices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmesh_vertices\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmesh_faces\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmesh_faces\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdensity\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdensity\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_acc\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Programming\\geodesyNets\\gravann\\_validation.py:42\u001B[0m, in \u001B[0;36mcompute_c_for_model_v2\u001B[1;34m(model, encoding, method, use_acc, **kwargs)\u001B[0m\n\u001B[0;32m     39\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     40\u001B[0m         mascon_points, mascon_masses, mascon_masses_nu \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmascon_points\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m), kwargs\u001B[38;5;241m.\u001B[39mget(\n\u001B[0;32m     41\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmascon_masses\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m), kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmascon_masses_nu\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m---> 42\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcompute_c_for_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmascon_points\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmascon_masses\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmascon_masses_nu\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_acc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m method \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpolyhedral\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m     44\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m sample \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\Programming\\geodesyNets\\gravann\\_validation.py:67\u001B[0m, in \u001B[0;36mcompute_c_for_model\u001B[1;34m(model, encoding, mascon_points, mascon_masses, mascon_masses_nu, use_acc)\u001B[0m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_acc:\n\u001B[0;32m     66\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m mascon_masses_nu \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 67\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_compute_c_for_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     68\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mMASCON_ACC_L\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmascon_points\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmascon_masses\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     69\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mACC_trap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mN\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100000\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     70\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     71\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m _compute_c_for_model(\n\u001B[0;32m     73\u001B[0m             \u001B[38;5;28;01mlambda\u001B[39;00m x: MASCON_ACC_L_differential(x, mascon_points, mascon_masses, mascon_masses_nu),\n\u001B[0;32m     74\u001B[0m             \u001B[38;5;28;01mlambda\u001B[39;00m x: ACC_trap(x, model, encoding, N\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100000\u001B[39m)\n\u001B[0;32m     75\u001B[0m         )\n",
      "File \u001B[1;32m~\\Programming\\geodesyNets\\gravann\\_validation.py:115\u001B[0m, in \u001B[0;36m_compute_c_for_model\u001B[1;34m(label, label_trap)\u001B[0m\n\u001B[0;32m    113\u001B[0m targets_point_sampler \u001B[38;5;241m=\u001B[39m get_target_point_sampler(\u001B[38;5;241m1000\u001B[39m, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspherical\u001B[39m\u001B[38;5;124m\"\u001B[39m, bounds\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m0.81\u001B[39m, \u001B[38;5;241m1.0\u001B[39m])\n\u001B[0;32m    114\u001B[0m target_points \u001B[38;5;241m=\u001B[39m targets_point_sampler()\n\u001B[1;32m--> 115\u001B[0m labels \u001B[38;5;241m=\u001B[39m \u001B[43mlabel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtarget_points\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    116\u001B[0m predicted \u001B[38;5;241m=\u001B[39m label_trap(target_points)\n\u001B[0;32m    117\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39msum(predicted \u001B[38;5;241m*\u001B[39m labels) \u001B[38;5;241m/\u001B[39m torch\u001B[38;5;241m.\u001B[39msum(predicted \u001B[38;5;241m*\u001B[39m predicted))\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\Programming\\geodesyNets\\gravann\\_validation.py:68\u001B[0m, in \u001B[0;36mcompute_c_for_model.<locals>.<lambda>\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_acc:\n\u001B[0;32m     66\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m mascon_masses_nu \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     67\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m _compute_c_for_model(\n\u001B[1;32m---> 68\u001B[0m             \u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[43mMASCON_ACC_L\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmascon_points\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmascon_masses\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[0;32m     69\u001B[0m             \u001B[38;5;28;01mlambda\u001B[39;00m x: ACC_trap(x, model, encoding, N\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100000\u001B[39m)\n\u001B[0;32m     70\u001B[0m         )\n\u001B[0;32m     71\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m _compute_c_for_model(\n\u001B[0;32m     73\u001B[0m             \u001B[38;5;28;01mlambda\u001B[39;00m x: MASCON_ACC_L_differential(x, mascon_points, mascon_masses, mascon_masses_nu),\n\u001B[0;32m     74\u001B[0m             \u001B[38;5;28;01mlambda\u001B[39;00m x: ACC_trap(x, model, encoding, N\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100000\u001B[39m)\n\u001B[0;32m     75\u001B[0m         )\n",
      "File \u001B[1;32m~\\Programming\\geodesyNets\\gravann\\_mascon_labels.py:56\u001B[0m, in \u001B[0;36mACC_L\u001B[1;34m(target_points, mascon_points, mascon_masses)\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;124;03mComputes the acceleration due to the mascon at the target points. (to be used as Label in the training)\u001B[39;00m\n\u001B[0;32m     46\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;124;03m    1-D array-like: a (N, 3) torch tensor containing the acceleration (G=1) at the target points\u001B[39;00m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mascon_masses \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 56\u001B[0m     mm \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([\u001B[38;5;241m1.\u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmascon_points\u001B[49m\u001B[43m)\u001B[49m] \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mlen\u001B[39m(mascon_points),\n\u001B[0;32m     57\u001B[0m                       device\u001B[38;5;241m=\u001B[39mos\u001B[38;5;241m.\u001B[39menviron[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTORCH_DEVICE\u001B[39m\u001B[38;5;124m\"\u001B[39m])\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(mascon_masses) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28mint\u001B[39m:\n\u001B[0;32m     59\u001B[0m     mm \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([mascon_masses] \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mlen\u001B[39m(mascon_points),\n\u001B[0;32m     60\u001B[0m                       device\u001B[38;5;241m=\u001B[39mos\u001B[38;5;241m.\u001B[39menviron[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTORCH_DEVICE\u001B[39m\u001B[38;5;124m\"\u001B[39m])\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[1;31mTypeError\u001B[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# Encoding: direct encoding (i.e. feeding the network directly with the Cartesian coordinates in the unit hypercube)\n",
    "# was found to work well in most cases. But more options are implemented in the module.\n",
    "encoding = gravann.direct_encoding()\n",
    "\n",
    "# The model is here a SIREN network (FFNN with sin non linearities and a final absolute value to predict the density)\n",
    "model = gravann.init_network(encoding, n_neurons=100, model_type=\"siren\", activation = gravann.AbsLayer())\n",
    "#model = gravann.init_network(encoding, n_neurons=100, model_type=\"siren\", activation = gravann.SquaredReLU())\n",
    "#model = gravann.init_network(encoding, n_neurons=100, model_type=\"siren\", activation = torch.nn.ReLU())\n",
    "\n",
    "\n",
    "# Init training logs\n",
    "loss_log = []\n",
    "weighted_average_log = []\n",
    "running_loss_log = []\n",
    "n_inferences = []\n",
    "weighted_average = deque([], maxlen=20)\n",
    "\n",
    "if LOAD_TRAINED:\n",
    "    model.load_state_dict(torch.load(MODEL_FILENAME))\n",
    "\n",
    "# Once a model is loaded the learned constant c (named kappa in the paper) is unknown\n",
    "# and must be relearned (ideally it should also be saved at the end of the training as it is a learned parameter)\n",
    "c = gravann.compute_c_for_model_v2(model, encoding, MODEL_METHOD,\n",
    "                                   mascon_points=mascon_points,\n",
    "                                   mascon_masses=mascon_masses,\n",
    "                                   mesh_vertices=mesh_vertices,\n",
    "                                   mesh_faces=mesh_faces,\n",
    "                                   density=density,\n",
    "                                   use_acc = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Visualizing the initial neural density field\n",
    "The network output is a mass density in the unit cube. Essentially a three dimensional function which we here plot via rejection sampling (for now this method is good enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rejection plot will visualize the neural density field as a probability distribution function. \n",
    "# Colors represent the density magnitude (white being zero). \n",
    "# At the beginning, the whole hypercube is filled with some non vanishing density.\n",
    "gravann.plot_model_rejection(model, encoding, views_2d=True, N=10000, progressbar=True, c=c)\n",
    "plt.title(\"Believe it or not I am an asteroid!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Training of a geodesyNet\n",
    "Specify parameters if you're unhappy with the defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### PLEASE SPECIFY THIS ####################\n",
    "# Number of iterations to train\n",
    "NUMBER_OF_ITERATIONS = 10000\n",
    "\n",
    "# Number of points to be used to evaluate numerically the triple integral\n",
    "# defining the acceleration. \n",
    "# Use <=30000 to for a quick training ... 300000 was used to produce most of the paper results\n",
    "N_QUADRATUR = 10000\n",
    "\n",
    "# Dimension of the batch size, i.e. number of points\n",
    "# where the ground truth is compared to the predicted acceleration\n",
    "# at each training epoch.\n",
    "# Use 100 for a quick training. 1000  was used to produce most of the paper results\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "# Loss. The normalized L1 loss (kMAE in the paper) was\n",
    "# found to be one of the best performing choices.\n",
    "# More are implemented in the module\n",
    "LOSS_FN = gravann.normalized_L1_loss\n",
    "\n",
    "# The numerical Integration method. \n",
    "# Trapezoidal integration is here set over a dataset containing acceleration values,\n",
    "# (it is possible to also train on values of the gravity potential, results are similar)\n",
    "MC_METHOD = gravann.ACC_trap\n",
    "\n",
    "# Here we set the optimizer\n",
    "LEARNING_RATE = 1e-4\n",
    "OPTIMIZER = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "SCHEDULER = torch.optim.lr_scheduler.ReduceLROnPlateau(OPTIMIZER,factor = 0.8, patience = 200, min_lr = 1e-6,verbose=True)\n",
    "\n",
    "##############################################################\n",
    "\n",
    "LABEL_FUNCTIONS = {\n",
    "    'polyhedral': lambda points: gravann.polyhedral.ACC_L(points, mesh_vertices, mesh_faces, density),\n",
    "    'mascon': lambda points: gravann.ACC_L(points, mascon_points, mascon_masses)\n",
    "}\n",
    "\n",
    "label_fn = LABEL_FUNCTIONS[MODEL_METHOD]\n",
    "\n",
    "# And init the best results\n",
    "best_loss = np.inf\n",
    "best_model_state_dict = model.state_dict()\n",
    "\n",
    "# The sampling method to decide what points to consider in each batch.\n",
    "# In this case we sample points unifromly in a sphere and reject those that are inside the asteroid\n",
    "# Only for sampling the low poly resolution!!\n",
    "targets_point_sampler = gravann.get_target_point_sampler(BATCH_SIZE,\n",
    "                                                         limit_shape_to_asteroid=f\"./3dmeshes/{SAMPLE_NAME}_lp.pk\",\n",
    "                                                         method=\"spherical\",\n",
    "                                                         bounds=[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: the cell below is explicitly typed for convenience, as this is a tutorial-ish after all, but the module gravann contains a function (train_on_batch) that does the same**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TRAINING LOOP (normal training, no use of any prior shape information)------------------------\n",
    "# This cell can be stopped and started again without loosing memory of the training nor its logs\n",
    "torch.cuda.empty_cache()\n",
    "# The main training loop\n",
    "for i in range(NUMBER_OF_ITERATIONS):\n",
    "    # Each ten epochs we resample the target points\n",
    "    if (i % 10 == 0):\n",
    "        target_points = targets_point_sampler()\n",
    "        # We compute the labels whenever the target points are changed\n",
    "        labels = label_fn(target_points)\n",
    "    \n",
    "    # We compute the values predicted by the neural density field\n",
    "    predicted = MC_METHOD(target_points, model, encoding, N=N_QUADRATUR, noise=0.)\n",
    "    \n",
    "    # We learn the scaling constant (k in the paper)\n",
    "    c = torch.sum(predicted*labels)/torch.sum(predicted*predicted)\n",
    "    \n",
    "    # We compute the loss (note that the contrastive loss needs a different shape for the labels)\n",
    "    if LOSS_FN == gravann.contrastive_loss:\n",
    "       loss = LOSS_FN(predicted, labels)\n",
    "    else:\n",
    "       loss = LOSS_FN(predicted.view(-1), labels.view(-1))\n",
    "    \n",
    "    # We store the model if it has the lowest fitness \n",
    "    # (this is to avoid losing good results during a run that goes wild)\n",
    "    if loss < best_loss:\n",
    "        best_model_state_dict = model.state_dict()\n",
    "        best_loss = loss\n",
    "        print('New Best: ', loss.item())\n",
    "        # Uncomment to save the model during training (careful it overwrites the model folder)\n",
    "        #torch.save(model.state_dict(), \"models/\"+name_of_gt+\".mdl\")\n",
    "    \n",
    "    # Update the loss trend indicators\n",
    "    weighted_average.append(loss.item())\n",
    "    \n",
    "    # Update the logs\n",
    "    weighted_average_log.append(np.mean(weighted_average))\n",
    "    loss_log.append(loss.item())\n",
    "    n_inferences.append((N_QUADRATUR*BATCH_SIZE) // 1000000) #counted in millions\n",
    "    \n",
    "    # Print every i iterations\n",
    "    if i % 25 == 0:\n",
    "        wa_out = np.mean(weighted_average)\n",
    "        print(f\"It={i}\\t loss={loss.item():.3e}\\t  weighted_average={wa_out:.3e}\\t  c={c:.3e}\")\n",
    "        \n",
    "    # Zeroes the gradient (necessary because of things)\n",
    "    OPTIMIZER.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    OPTIMIZER.step()\n",
    "    \n",
    "    # Perform a step in LR scheduler to update LR\n",
    "    SCHEDULER.step(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we restore the learned parameters of the best model of the run\n",
    "for layer in model.state_dict():\n",
    "    model.state_dict()[layer] = best_model_state_dict[layer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Interpretation of the neural density field learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# First lets have a look at the training loss history\n",
    "plt.figure()\n",
    "abscissa = np.cumsum(n_inferences)\n",
    "plt.semilogy(abscissa,  loss_log)\n",
    "plt.semilogy(abscissa,  weighted_average_log)\n",
    "plt.xlabel(\"Thousands of model evaluations x 10\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"Loss\",\"Weighted Average Loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Lets have a look at the neural density field.\n",
    "# First with a rejection plot\n",
    "gravann.plot_model_rejection(model, encoding, views_2d=True, bw=True, N=1500, alpha=0.1, s=50, c=c, crop_p=0.1, progressbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save the model\n",
    "torch.save(model.state_dict(), MODEL_FILENAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
