{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Loading results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gravann.io import csv_input_loader\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "FILEPATH = \"F:\\\\results\"\n",
    "\n",
    "df = csv_input_loader.read_result_csv(FILEPATH, dir_substring=\"polyhedral-distance\")\n",
    "df[\"Seed\"] = df[\"Seed\"].astype(int)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "   Ground Truth Target Sampler Domain                     Loss     0.001  \\\n0    polyhedral            [0.0, 1.0]          normalized_loss  0.566179   \n1    polyhedral            [1.0, 3.0]          normalized_loss  0.932906   \n2    polyhedral            [3.0, 5.0]          normalized_loss  1.514046   \n3    polyhedral            [0.0, 1.0]  normalized_sqrt_L1_loss  0.433649   \n4    polyhedral            [1.0, 3.0]  normalized_sqrt_L1_loss  0.609579   \n5    polyhedral            [3.0, 5.0]  normalized_sqrt_L1_loss  0.618389   \n6    polyhedral            [0.0, 1.0]       normalized_L1_loss  0.038837   \n7    polyhedral            [1.0, 3.0]       normalized_L1_loss  0.681435   \n8    polyhedral            [3.0, 5.0]       normalized_L1_loss  0.662330   \n9        mascon            [0.0, 1.0]          normalized_loss  0.084012   \n10       mascon            [1.0, 3.0]          normalized_loss  0.846465   \n11       mascon            [3.0, 5.0]          normalized_loss  1.495812   \n12       mascon            [0.0, 1.0]  normalized_sqrt_L1_loss  0.072985   \n13       mascon            [1.0, 3.0]  normalized_sqrt_L1_loss  0.792880   \n14       mascon            [3.0, 5.0]  normalized_sqrt_L1_loss  0.615515   \n15       mascon            [0.0, 1.0]       normalized_L1_loss  0.031383   \n16       mascon            [1.0, 3.0]       normalized_L1_loss  0.681798   \n17       mascon            [3.0, 5.0]       normalized_L1_loss  0.638051   \n0    polyhedral            [0.0, 1.0]          normalized_loss  0.090333   \n1    polyhedral            [1.0, 3.0]          normalized_loss  0.806220   \n2    polyhedral            [3.0, 5.0]          normalized_loss  0.821026   \n3    polyhedral            [0.0, 1.0]  normalized_sqrt_L1_loss  0.089096   \n4    polyhedral            [1.0, 3.0]  normalized_sqrt_L1_loss  0.798569   \n5    polyhedral            [3.0, 5.0]  normalized_sqrt_L1_loss  1.501677   \n6    polyhedral            [0.0, 1.0]       normalized_L1_loss  0.033296   \n7    polyhedral            [1.0, 3.0]       normalized_L1_loss  0.707776   \n8    polyhedral            [3.0, 5.0]       normalized_L1_loss  0.591667   \n9        mascon            [0.0, 1.0]          normalized_loss  0.081553   \n10       mascon            [1.0, 3.0]          normalized_loss  0.825448   \n11       mascon            [3.0, 5.0]          normalized_loss  1.160876   \n12       mascon            [0.0, 1.0]  normalized_sqrt_L1_loss  0.087123   \n13       mascon            [1.0, 3.0]  normalized_sqrt_L1_loss  1.506348   \n14       mascon            [3.0, 5.0]  normalized_sqrt_L1_loss  0.724652   \n15       mascon            [0.0, 1.0]       normalized_L1_loss  0.241820   \n16       mascon            [1.0, 3.0]       normalized_L1_loss  0.670941   \n17       mascon            [3.0, 5.0]       normalized_L1_loss  0.604972   \n18   polyhedral            [0.0, 1.0]          normalized_loss  0.186276   \n19   polyhedral            [1.0, 3.0]          normalized_loss  1.487702   \n20   polyhedral            [3.0, 5.0]          normalized_loss  0.835824   \n21   polyhedral            [0.0, 1.0]  normalized_sqrt_L1_loss  0.071459   \n22   polyhedral            [1.0, 3.0]  normalized_sqrt_L1_loss  0.570982   \n23   polyhedral            [3.0, 5.0]  normalized_sqrt_L1_loss  0.597866   \n24   polyhedral            [0.0, 1.0]       normalized_L1_loss  0.155223   \n25   polyhedral            [1.0, 3.0]       normalized_L1_loss  0.703236   \n26   polyhedral            [3.0, 5.0]       normalized_L1_loss  0.966088   \n27       mascon            [0.0, 1.0]          normalized_loss  0.077850   \n28       mascon            [1.0, 3.0]          normalized_loss  0.836672   \n29       mascon            [3.0, 5.0]          normalized_loss  0.865148   \n30       mascon            [0.0, 1.0]  normalized_sqrt_L1_loss  0.090509   \n31       mascon            [1.0, 3.0]  normalized_sqrt_L1_loss  1.505642   \n32       mascon            [3.0, 5.0]  normalized_sqrt_L1_loss  0.743794   \n33       mascon            [0.0, 1.0]       normalized_L1_loss  0.088648   \n34       mascon            [1.0, 3.0]       normalized_L1_loss  1.184289   \n35       mascon            [3.0, 5.0]       normalized_L1_loss  0.660399   \n\n        0.05       1.0       2.0  \n0   0.030502  0.028974  0.004515  \n1   0.831791  0.012540  0.000215  \n2   0.829988  0.021097  0.000667  \n3   0.010284  0.000945  0.000542  \n4   0.509265  0.022407  0.000361  \n5   0.605409  0.034100  0.001282  \n6   0.004623  0.000287  0.000137  \n7   0.616884  0.009644  0.000268  \n8   0.576780  0.028957  0.000809  \n9   0.029141  0.018281  0.003482  \n10  0.766504  0.005954  0.000108  \n11  0.908411  0.019687  0.000645  \n12  0.012208  0.001570  0.000990  \n13  0.601227  0.015535  0.000435  \n14  0.594121  0.033310  0.000782  \n15  0.006099  0.000733  0.000421  \n16  0.738787  0.007128  0.000255  \n17  0.647521  0.023438  0.000840  \n0   0.030442  0.016995  0.002679  \n1   0.655128  0.009618  0.000053  \n2   0.968411  0.034395  0.000745  \n3   0.011985  0.000767  0.000366  \n4   0.778717  0.019203  0.000382  \n5   0.703692  0.024309  0.000946  \n6   0.004020  0.000568  0.000210  \n7   0.615798  0.010915  0.000217  \n8   0.834386  0.025311  0.000677  \n9   0.028231  0.017214  0.002919  \n10  0.620568  0.015532  0.000049  \n11  0.868699  0.028909  0.000793  \n12  0.013673  0.000897  0.000400  \n13  0.599920  0.016658  0.000513  \n14  0.520612  0.021703  0.001081  \n15  0.006581  0.000848  0.000472  \n16  0.612367  0.014611  0.000288  \n17  0.603636  0.019063  0.000893  \n18  0.020496  0.020367  0.002900  \n19  0.644115  0.008391  0.000051  \n20  0.905499  0.025791  0.000868  \n21  0.016773  0.000678  0.000248  \n22  0.553674  0.017289  0.000419  \n23  0.540779  0.022605  0.000906  \n24  0.003373  0.000665  0.000316  \n25  0.606653  0.014908  0.000250  \n26  0.641736  0.020531  0.000581  \n27  0.027815  0.017172  0.003542  \n28  0.652978  0.006943  0.000180  \n29  0.752343  0.027916  0.001079  \n30  0.014928  0.000831  0.000401  \n31  0.533040  0.016565  0.000301  \n32  0.571417  0.021702  0.000996  \n33  0.005601  0.000550  0.000284  \n34  0.597592  0.009730  0.000368  \n35  0.625587  0.022897  0.000722  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Ground Truth</th>\n      <th>Target Sampler Domain</th>\n      <th>Loss</th>\n      <th>0.001</th>\n      <th>0.05</th>\n      <th>1.0</th>\n      <th>2.0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>polyhedral</td>\n      <td>[0.0, 1.0]</td>\n      <td>normalized_loss</td>\n      <td>0.566179</td>\n      <td>0.030502</td>\n      <td>0.028974</td>\n      <td>0.004515</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>polyhedral</td>\n      <td>[1.0, 3.0]</td>\n      <td>normalized_loss</td>\n      <td>0.932906</td>\n      <td>0.831791</td>\n      <td>0.012540</td>\n      <td>0.000215</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>polyhedral</td>\n      <td>[3.0, 5.0]</td>\n      <td>normalized_loss</td>\n      <td>1.514046</td>\n      <td>0.829988</td>\n      <td>0.021097</td>\n      <td>0.000667</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>polyhedral</td>\n      <td>[0.0, 1.0]</td>\n      <td>normalized_sqrt_L1_loss</td>\n      <td>0.433649</td>\n      <td>0.010284</td>\n      <td>0.000945</td>\n      <td>0.000542</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>polyhedral</td>\n      <td>[1.0, 3.0]</td>\n      <td>normalized_sqrt_L1_loss</td>\n      <td>0.609579</td>\n      <td>0.509265</td>\n      <td>0.022407</td>\n      <td>0.000361</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>polyhedral</td>\n      <td>[3.0, 5.0]</td>\n      <td>normalized_sqrt_L1_loss</td>\n      <td>0.618389</td>\n      <td>0.605409</td>\n      <td>0.034100</td>\n      <td>0.001282</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>polyhedral</td>\n      <td>[0.0, 1.0]</td>\n      <td>normalized_L1_loss</td>\n      <td>0.038837</td>\n      <td>0.004623</td>\n      <td>0.000287</td>\n      <td>0.000137</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>polyhedral</td>\n      <td>[1.0, 3.0]</td>\n      <td>normalized_L1_loss</td>\n      <td>0.681435</td>\n      <td>0.616884</td>\n      <td>0.009644</td>\n      <td>0.000268</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>polyhedral</td>\n      <td>[3.0, 5.0]</td>\n      <td>normalized_L1_loss</td>\n      <td>0.662330</td>\n      <td>0.576780</td>\n      <td>0.028957</td>\n      <td>0.000809</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>mascon</td>\n      <td>[0.0, 1.0]</td>\n      <td>normalized_loss</td>\n      <td>0.084012</td>\n      <td>0.029141</td>\n      <td>0.018281</td>\n      <td>0.003482</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>mascon</td>\n      <td>[1.0, 3.0]</td>\n      <td>normalized_loss</td>\n      <td>0.846465</td>\n      <td>0.766504</td>\n      <td>0.005954</td>\n      <td>0.000108</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>mascon</td>\n      <td>[3.0, 5.0]</td>\n      <td>normalized_loss</td>\n      <td>1.495812</td>\n      <td>0.908411</td>\n      <td>0.019687</td>\n      <td>0.000645</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>mascon</td>\n      <td>[0.0, 1.0]</td>\n      <td>normalized_sqrt_L1_loss</td>\n      <td>0.072985</td>\n      <td>0.012208</td>\n      <td>0.001570</td>\n      <td>0.000990</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>mascon</td>\n      <td>[1.0, 3.0]</td>\n      <td>normalized_sqrt_L1_loss</td>\n      <td>0.792880</td>\n      <td>0.601227</td>\n      <td>0.015535</td>\n      <td>0.000435</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>mascon</td>\n      <td>[3.0, 5.0]</td>\n      <td>normalized_sqrt_L1_loss</td>\n      <td>0.615515</td>\n      <td>0.594121</td>\n      <td>0.033310</td>\n      <td>0.000782</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>mascon</td>\n      <td>[0.0, 1.0]</td>\n      <td>normalized_L1_loss</td>\n      <td>0.031383</td>\n      <td>0.006099</td>\n      <td>0.000733</td>\n      <td>0.000421</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>mascon</td>\n      <td>[1.0, 3.0]</td>\n      <td>normalized_L1_loss</td>\n      <td>0.681798</td>\n      <td>0.738787</td>\n      <td>0.007128</td>\n      <td>0.000255</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>mascon</td>\n      <td>[3.0, 5.0]</td>\n      <td>normalized_L1_loss</td>\n      <td>0.638051</td>\n      <td>0.647521</td>\n      <td>0.023438</td>\n      <td>0.000840</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>polyhedral</td>\n      <td>[0.0, 1.0]</td>\n      <td>normalized_loss</td>\n      <td>0.090333</td>\n      <td>0.030442</td>\n      <td>0.016995</td>\n      <td>0.002679</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>polyhedral</td>\n      <td>[1.0, 3.0]</td>\n      <td>normalized_loss</td>\n      <td>0.806220</td>\n      <td>0.655128</td>\n      <td>0.009618</td>\n      <td>0.000053</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>polyhedral</td>\n      <td>[3.0, 5.0]</td>\n      <td>normalized_loss</td>\n      <td>0.821026</td>\n      <td>0.968411</td>\n      <td>0.034395</td>\n      <td>0.000745</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>polyhedral</td>\n      <td>[0.0, 1.0]</td>\n      <td>normalized_sqrt_L1_loss</td>\n      <td>0.089096</td>\n      <td>0.011985</td>\n      <td>0.000767</td>\n      <td>0.000366</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>polyhedral</td>\n      <td>[1.0, 3.0]</td>\n      <td>normalized_sqrt_L1_loss</td>\n      <td>0.798569</td>\n      <td>0.778717</td>\n      <td>0.019203</td>\n      <td>0.000382</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>polyhedral</td>\n      <td>[3.0, 5.0]</td>\n      <td>normalized_sqrt_L1_loss</td>\n      <td>1.501677</td>\n      <td>0.703692</td>\n      <td>0.024309</td>\n      <td>0.000946</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>polyhedral</td>\n      <td>[0.0, 1.0]</td>\n      <td>normalized_L1_loss</td>\n      <td>0.033296</td>\n      <td>0.004020</td>\n      <td>0.000568</td>\n      <td>0.000210</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>polyhedral</td>\n      <td>[1.0, 3.0]</td>\n      <td>normalized_L1_loss</td>\n      <td>0.707776</td>\n      <td>0.615798</td>\n      <td>0.010915</td>\n      <td>0.000217</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>polyhedral</td>\n      <td>[3.0, 5.0]</td>\n      <td>normalized_L1_loss</td>\n      <td>0.591667</td>\n      <td>0.834386</td>\n      <td>0.025311</td>\n      <td>0.000677</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>mascon</td>\n      <td>[0.0, 1.0]</td>\n      <td>normalized_loss</td>\n      <td>0.081553</td>\n      <td>0.028231</td>\n      <td>0.017214</td>\n      <td>0.002919</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>mascon</td>\n      <td>[1.0, 3.0]</td>\n      <td>normalized_loss</td>\n      <td>0.825448</td>\n      <td>0.620568</td>\n      <td>0.015532</td>\n      <td>0.000049</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>mascon</td>\n      <td>[3.0, 5.0]</td>\n      <td>normalized_loss</td>\n      <td>1.160876</td>\n      <td>0.868699</td>\n      <td>0.028909</td>\n      <td>0.000793</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>mascon</td>\n      <td>[0.0, 1.0]</td>\n      <td>normalized_sqrt_L1_loss</td>\n      <td>0.087123</td>\n      <td>0.013673</td>\n      <td>0.000897</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>mascon</td>\n      <td>[1.0, 3.0]</td>\n      <td>normalized_sqrt_L1_loss</td>\n      <td>1.506348</td>\n      <td>0.599920</td>\n      <td>0.016658</td>\n      <td>0.000513</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>mascon</td>\n      <td>[3.0, 5.0]</td>\n      <td>normalized_sqrt_L1_loss</td>\n      <td>0.724652</td>\n      <td>0.520612</td>\n      <td>0.021703</td>\n      <td>0.001081</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>mascon</td>\n      <td>[0.0, 1.0]</td>\n      <td>normalized_L1_loss</td>\n      <td>0.241820</td>\n      <td>0.006581</td>\n      <td>0.000848</td>\n      <td>0.000472</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>mascon</td>\n      <td>[1.0, 3.0]</td>\n      <td>normalized_L1_loss</td>\n      <td>0.670941</td>\n      <td>0.612367</td>\n      <td>0.014611</td>\n      <td>0.000288</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>mascon</td>\n      <td>[3.0, 5.0]</td>\n      <td>normalized_L1_loss</td>\n      <td>0.604972</td>\n      <td>0.603636</td>\n      <td>0.019063</td>\n      <td>0.000893</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>polyhedral</td>\n      <td>[0.0, 1.0]</td>\n      <td>normalized_loss</td>\n      <td>0.186276</td>\n      <td>0.020496</td>\n      <td>0.020367</td>\n      <td>0.002900</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>polyhedral</td>\n      <td>[1.0, 3.0]</td>\n      <td>normalized_loss</td>\n      <td>1.487702</td>\n      <td>0.644115</td>\n      <td>0.008391</td>\n      <td>0.000051</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>polyhedral</td>\n      <td>[3.0, 5.0]</td>\n      <td>normalized_loss</td>\n      <td>0.835824</td>\n      <td>0.905499</td>\n      <td>0.025791</td>\n      <td>0.000868</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>polyhedral</td>\n      <td>[0.0, 1.0]</td>\n      <td>normalized_sqrt_L1_loss</td>\n      <td>0.071459</td>\n      <td>0.016773</td>\n      <td>0.000678</td>\n      <td>0.000248</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>polyhedral</td>\n      <td>[1.0, 3.0]</td>\n      <td>normalized_sqrt_L1_loss</td>\n      <td>0.570982</td>\n      <td>0.553674</td>\n      <td>0.017289</td>\n      <td>0.000419</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>polyhedral</td>\n      <td>[3.0, 5.0]</td>\n      <td>normalized_sqrt_L1_loss</td>\n      <td>0.597866</td>\n      <td>0.540779</td>\n      <td>0.022605</td>\n      <td>0.000906</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>polyhedral</td>\n      <td>[0.0, 1.0]</td>\n      <td>normalized_L1_loss</td>\n      <td>0.155223</td>\n      <td>0.003373</td>\n      <td>0.000665</td>\n      <td>0.000316</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>polyhedral</td>\n      <td>[1.0, 3.0]</td>\n      <td>normalized_L1_loss</td>\n      <td>0.703236</td>\n      <td>0.606653</td>\n      <td>0.014908</td>\n      <td>0.000250</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>polyhedral</td>\n      <td>[3.0, 5.0]</td>\n      <td>normalized_L1_loss</td>\n      <td>0.966088</td>\n      <td>0.641736</td>\n      <td>0.020531</td>\n      <td>0.000581</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>mascon</td>\n      <td>[0.0, 1.0]</td>\n      <td>normalized_loss</td>\n      <td>0.077850</td>\n      <td>0.027815</td>\n      <td>0.017172</td>\n      <td>0.003542</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>mascon</td>\n      <td>[1.0, 3.0]</td>\n      <td>normalized_loss</td>\n      <td>0.836672</td>\n      <td>0.652978</td>\n      <td>0.006943</td>\n      <td>0.000180</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>mascon</td>\n      <td>[3.0, 5.0]</td>\n      <td>normalized_loss</td>\n      <td>0.865148</td>\n      <td>0.752343</td>\n      <td>0.027916</td>\n      <td>0.001079</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>mascon</td>\n      <td>[0.0, 1.0]</td>\n      <td>normalized_sqrt_L1_loss</td>\n      <td>0.090509</td>\n      <td>0.014928</td>\n      <td>0.000831</td>\n      <td>0.000401</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>mascon</td>\n      <td>[1.0, 3.0]</td>\n      <td>normalized_sqrt_L1_loss</td>\n      <td>1.505642</td>\n      <td>0.533040</td>\n      <td>0.016565</td>\n      <td>0.000301</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>mascon</td>\n      <td>[3.0, 5.0]</td>\n      <td>normalized_sqrt_L1_loss</td>\n      <td>0.743794</td>\n      <td>0.571417</td>\n      <td>0.021702</td>\n      <td>0.000996</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>mascon</td>\n      <td>[0.0, 1.0]</td>\n      <td>normalized_L1_loss</td>\n      <td>0.088648</td>\n      <td>0.005601</td>\n      <td>0.000550</td>\n      <td>0.000284</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>mascon</td>\n      <td>[1.0, 3.0]</td>\n      <td>normalized_L1_loss</td>\n      <td>1.184289</td>\n      <td>0.597592</td>\n      <td>0.009730</td>\n      <td>0.000368</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>mascon</td>\n      <td>[3.0, 5.0]</td>\n      <td>normalized_L1_loss</td>\n      <td>0.660399</td>\n      <td>0.625587</td>\n      <td>0.022897</td>\n      <td>0.000722</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rel = df[[\"Seed\", \"Ground Truth\", \"Target Sampler Domain\", \"Loss\", \"relRMSE@Altitude_0\", \"relRMSE@Altitude_1\", \"relRMSE@Altitude_2\", \"relRMSE@Altitude_3\"]]\n",
    "\n",
    "df_rel.columns = [\"Seed\", \"Ground Truth\", \"Target Sampler Domain\", \"Loss\", \"0.001\", \"0.05\", \"1.0\", \"2.0\"]\n",
    "\n",
    "df_rel = df_rel.drop(columns=[\"Seed\"])\n",
    "df_rel"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                               0.001  \\\nGround Truth Target Sampler Domain Loss                                \nmascon       [0.0, 1.0]            normalized_L1_loss       0.120617   \n                                   normalized_loss          0.081138   \n                                   normalized_sqrt_L1_loss  0.083539   \n             [1.0, 3.0]            normalized_L1_loss       0.845676   \n                                   normalized_loss          0.836195   \n                                   normalized_sqrt_L1_loss  1.268290   \n             [3.0, 5.0]            normalized_L1_loss       0.634474   \n                                   normalized_loss          1.173945   \n                                   normalized_sqrt_L1_loss  0.694653   \npolyhedral   [0.0, 1.0]            normalized_L1_loss       0.075785   \n                                   normalized_loss          0.280929   \n                                   normalized_sqrt_L1_loss  0.198068   \n             [1.0, 3.0]            normalized_L1_loss       0.697482   \n                                   normalized_loss          1.075610   \n                                   normalized_sqrt_L1_loss  0.659710   \n             [3.0, 5.0]            normalized_L1_loss       0.740029   \n                                   normalized_loss          1.056965   \n                                   normalized_sqrt_L1_loss  0.905977   \n\n                                                                0.05  \\\nGround Truth Target Sampler Domain Loss                                \nmascon       [0.0, 1.0]            normalized_L1_loss       0.006094   \n                                   normalized_loss          0.028396   \n                                   normalized_sqrt_L1_loss  0.013603   \n             [1.0, 3.0]            normalized_L1_loss       0.649582   \n                                   normalized_loss          0.680017   \n                                   normalized_sqrt_L1_loss  0.578062   \n             [3.0, 5.0]            normalized_L1_loss       0.625581   \n                                   normalized_loss          0.843151   \n                                   normalized_sqrt_L1_loss  0.562050   \npolyhedral   [0.0, 1.0]            normalized_L1_loss       0.004005   \n                                   normalized_loss          0.027147   \n                                   normalized_sqrt_L1_loss  0.013014   \n             [1.0, 3.0]            normalized_L1_loss       0.613111   \n                                   normalized_loss          0.710345   \n                                   normalized_sqrt_L1_loss  0.613885   \n             [3.0, 5.0]            normalized_L1_loss       0.684301   \n                                   normalized_loss          0.901299   \n                                   normalized_sqrt_L1_loss  0.616627   \n\n                                                                 1.0       2.0  \nGround Truth Target Sampler Domain Loss                                         \nmascon       [0.0, 1.0]            normalized_L1_loss       0.000710  0.000393  \n                                   normalized_loss          0.017556  0.003314  \n                                   normalized_sqrt_L1_loss  0.001099  0.000597  \n             [1.0, 3.0]            normalized_L1_loss       0.010490  0.000304  \n                                   normalized_loss          0.009476  0.000112  \n                                   normalized_sqrt_L1_loss  0.016253  0.000417  \n             [3.0, 5.0]            normalized_L1_loss       0.021799  0.000819  \n                                   normalized_loss          0.025504  0.000839  \n                                   normalized_sqrt_L1_loss  0.025572  0.000953  \npolyhedral   [0.0, 1.0]            normalized_L1_loss       0.000507  0.000221  \n                                   normalized_loss          0.022112  0.003365  \n                                   normalized_sqrt_L1_loss  0.000796  0.000385  \n             [1.0, 3.0]            normalized_L1_loss       0.011823  0.000245  \n                                   normalized_loss          0.010183  0.000106  \n                                   normalized_sqrt_L1_loss  0.019633  0.000387  \n             [3.0, 5.0]            normalized_L1_loss       0.024933  0.000689  \n                                   normalized_loss          0.027094  0.000760  \n                                   normalized_sqrt_L1_loss  0.027005  0.001045  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>0.001</th>\n      <th>0.05</th>\n      <th>1.0</th>\n      <th>2.0</th>\n    </tr>\n    <tr>\n      <th>Ground Truth</th>\n      <th>Target Sampler Domain</th>\n      <th>Loss</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"9\" valign=\"top\">mascon</th>\n      <th rowspan=\"3\" valign=\"top\">[0.0, 1.0]</th>\n      <th>normalized_L1_loss</th>\n      <td>0.120617</td>\n      <td>0.006094</td>\n      <td>0.000710</td>\n      <td>0.000393</td>\n    </tr>\n    <tr>\n      <th>normalized_loss</th>\n      <td>0.081138</td>\n      <td>0.028396</td>\n      <td>0.017556</td>\n      <td>0.003314</td>\n    </tr>\n    <tr>\n      <th>normalized_sqrt_L1_loss</th>\n      <td>0.083539</td>\n      <td>0.013603</td>\n      <td>0.001099</td>\n      <td>0.000597</td>\n    </tr>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">[1.0, 3.0]</th>\n      <th>normalized_L1_loss</th>\n      <td>0.845676</td>\n      <td>0.649582</td>\n      <td>0.010490</td>\n      <td>0.000304</td>\n    </tr>\n    <tr>\n      <th>normalized_loss</th>\n      <td>0.836195</td>\n      <td>0.680017</td>\n      <td>0.009476</td>\n      <td>0.000112</td>\n    </tr>\n    <tr>\n      <th>normalized_sqrt_L1_loss</th>\n      <td>1.268290</td>\n      <td>0.578062</td>\n      <td>0.016253</td>\n      <td>0.000417</td>\n    </tr>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">[3.0, 5.0]</th>\n      <th>normalized_L1_loss</th>\n      <td>0.634474</td>\n      <td>0.625581</td>\n      <td>0.021799</td>\n      <td>0.000819</td>\n    </tr>\n    <tr>\n      <th>normalized_loss</th>\n      <td>1.173945</td>\n      <td>0.843151</td>\n      <td>0.025504</td>\n      <td>0.000839</td>\n    </tr>\n    <tr>\n      <th>normalized_sqrt_L1_loss</th>\n      <td>0.694653</td>\n      <td>0.562050</td>\n      <td>0.025572</td>\n      <td>0.000953</td>\n    </tr>\n    <tr>\n      <th rowspan=\"9\" valign=\"top\">polyhedral</th>\n      <th rowspan=\"3\" valign=\"top\">[0.0, 1.0]</th>\n      <th>normalized_L1_loss</th>\n      <td>0.075785</td>\n      <td>0.004005</td>\n      <td>0.000507</td>\n      <td>0.000221</td>\n    </tr>\n    <tr>\n      <th>normalized_loss</th>\n      <td>0.280929</td>\n      <td>0.027147</td>\n      <td>0.022112</td>\n      <td>0.003365</td>\n    </tr>\n    <tr>\n      <th>normalized_sqrt_L1_loss</th>\n      <td>0.198068</td>\n      <td>0.013014</td>\n      <td>0.000796</td>\n      <td>0.000385</td>\n    </tr>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">[1.0, 3.0]</th>\n      <th>normalized_L1_loss</th>\n      <td>0.697482</td>\n      <td>0.613111</td>\n      <td>0.011823</td>\n      <td>0.000245</td>\n    </tr>\n    <tr>\n      <th>normalized_loss</th>\n      <td>1.075610</td>\n      <td>0.710345</td>\n      <td>0.010183</td>\n      <td>0.000106</td>\n    </tr>\n    <tr>\n      <th>normalized_sqrt_L1_loss</th>\n      <td>0.659710</td>\n      <td>0.613885</td>\n      <td>0.019633</td>\n      <td>0.000387</td>\n    </tr>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">[3.0, 5.0]</th>\n      <th>normalized_L1_loss</th>\n      <td>0.740029</td>\n      <td>0.684301</td>\n      <td>0.024933</td>\n      <td>0.000689</td>\n    </tr>\n    <tr>\n      <th>normalized_loss</th>\n      <td>1.056965</td>\n      <td>0.901299</td>\n      <td>0.027094</td>\n      <td>0.000760</td>\n    </tr>\n    <tr>\n      <th>normalized_sqrt_L1_loss</th>\n      <td>0.905977</td>\n      <td>0.616627</td>\n      <td>0.027005</td>\n      <td>0.001045</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rel.groupby([\"Ground Truth\", \"Target Sampler Domain\", \"Loss\"]).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
